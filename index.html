<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="POET: Prompt Offset Tuning for Continual Human Action Adaptation.">
    <meta name="keywords" content="Continual learning, human action recognition, few-shot learning, extended reality">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>POET: Prompt Offset Tuning for Continual Human Action Adaptation</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="https://use.typekit.net/iag3ven.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <style>
        /* Applying smaller font size to the abstract and setting fonts to Georgia and Garamond */
        .abstract-text {
            font-size: 16px;
            font-family: 'Georgia', 'Garamond', serif;
        }

        .section-header {
            font-size: 1.5rem;
            font-family: 'Georgia', 'Garamond', serif;
            font-weight: bold;
            text-align: center;
        }

        /* Styling for the subsection */
        .benchmark-section {
        background-color: #f0f0f0; /* Light gray background */
        padding: 20px;
        font-size: 14px; /* Setting font size to 14px */
        }

        .benchmark-section h2 {
        font-size: 1.5rem;
        font-weight: bold;
        }

            /* Image gallery styling to fit images in a row */
        .image-gallery {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 10px; /* Space between images */
        }

        /* Resize the images to fit within the container */
        .image-gallery img {
            max-width: 23%; /* Ensures four images fit in one row */
            height: auto;
            border-radius: 8px;
        }

        /* Responsive design for smaller screens */
        @media (max-width: 768px) {
            .image-gallery img {
                max-width: 48%; /* Two images per row on smaller screens */
            }
        }

        @media (max-width: 480px) {
            .image-gallery img {
                max-width: 100%; /* Full width images on mobile screens */
            }
        }

        /* New styles for author names and affiliations */
        .author-block, .affiliation-block {
            font-family: 'Georgia', sans-serif;
        }

        .affiliation-block {
            margin-right: 15px;
        }

        .list-item {
        list-style: "⦿ ";
        margin-left: 1.5rem;
        }

        .video-container {
        width: 100%;
        display: flex;
        justify-content: center;
        align-items: center;
        margin: 20px 0;
        }

        .video-wrapper {
        width: 70%;
        max-width: 1200px; /* Optional: sets a maximum width for very large screens */
        position: relative;
        padding-bottom: 25%; /* 16:9 aspect ratio */
        }

        .video-wrapper iframe {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        }

    </style>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">POET: Prompt Offset Tuning for Continual Human Action
                            Adaptation</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://prachigarg23.github.io/">Prachi Garg</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://josephkj.in/">Joseph K J</a><sup>3</sup>,</span>
                            <span class="author-block">
                                <a href="https://people.iith.ac.in/vineethnb/">Vineeth N
                                    Balasubramanian</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.cihancamgoz.com/">Necati Cihan Camgoz</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.ch/citations?user=bm5dZwIAAAAJ&hl=en">Chengde
                                    Wan</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/kenrick-kin-51575753">Kenrick Kin</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=QivUtBIAAAAJ&hl=en">Weiguang
                                    Si</a><sup>2</sup>
                            </span>
                            <span class="author-block">
                                <a href="https://shugaoma.github.io/">Shugao Ma</a><sup>2</sup>
                            </span>
                            <span class="author-block">
                                <a href="https://www.cs.cmu.edu/~ftorre/">Fernando De La Torre</a><sup>1</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="affiliation-block"><sup>1</sup>Carnegie Mellon University</span>
                            <span class="affiliation-block"><sup>2</sup>Meta</span>
                            <span class="affiliation-block"><sup>3</sup>Indian Institute of Technology Hyderabad</span>
                        </div>

                        <p style="padding: 12.5px;" />
                        <h1 class="conference">
                            ECCV 2024, Oral Presentation
                        </h1>
                        <p style="padding: 10px;" />

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="http://www.humansensing.cs.cmu.edu/sites/default/files/final_ECCV_POET_complete.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://github.com/humansensinglab/POET-continual-action-recognition"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://www.youtube.com/embed/oOK0cglVKQk" class="external-link button is-normal is-rounded is-dark">
                                      <span class="icon">
                                        <i class="fab fa-youtube"></i>
                                      </span>
                                      <span>Video</span>
                                    </a>
                                </span>
                                <!-- <span class="link-block"></span>
                                    <a href="static/docs/POET_ECCV_poster.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Poster</span>
                                    </a>
                                </span> -->
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <p style="text-align: center; padding: 12.5px;"><b>TL;DR: POET enables users to personalize their experience by adding new action classes
        efficiently and continually whenever they want.</b></p style="padding: 12.5px;">

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <video id="teaser" autoplay muted loop playsinline height="100%">
                    <source src="./static/videos/full_teaser_system.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </section>

    <p style="text-align: center;"><b>ECCV Talk Video</b></p>

    <div class="video-container">
        <div class="video-wrapper">
          <iframe src="https://www.youtube.com/embed/oOK0cglVKQk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
          </iframe>
        </div>
      </div>


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified abstract-text">
                        <p>
                            As extended reality (XR) is redefining how users interact with computing devices, research
                            in human action recognition
                            is gaining prominence. Typically, models deployed on immersive computing devices are static
                            and limited to their default
                            set of classes.
                        </p>
                        <p>
                            The goal of our research is to provide users and developers with the capability to
                            personalize their
                            experience by <b>adding new action classes to their device models continually</b>. Importantly, a
                            user should be able to add new classes in a <b>low-shot and efficient manner</b>, while this
                            process should
                            <b>not require storing or replaying any of user's sensitive training data</b>. We formalize this
                            problem as privacy-aware few-shot
                            continual action recognition.
                        </p>
                        <p>
                            Towards this end, we propose <b>POET</b>:<b>P</b>rompt <b>O</b>ffs<b>e</b>t <b>T</b>uning.
                            While existing prompt tuning approaches have
                            shown great promise for continual learning of image, text, and video modalities; they demand
                            access to extensively
                            pretrained transformers. Breaking away from this assumption, POET demonstrates the efficacy
                            of prompt tuning a
                            significantly lightweight backbone, pretrained exclusively on the base class data. We
                            propose a novel spatio-temporal
                            learnable prompt offset tuning approach, and are the first to apply such prompt tuning to
                            <i>Graph Neural
                                Networks</i>.
                        </p>
                        <p>
                            We contribute two new benchmarks for our new problem setting in human action recognition:
                            (i) NTU RGB+D
                            dataset for activity recognition, and (ii) SHREC-2017 dataset for hand gesture recognition.
                            We find that POET
                            consistently outperforms comprehensive benchmarks.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    
    <section class="section benchmark-section">
        <div class="container is-max-desktop">
            <!-- Image Row -->
            <h2 class="section-header">Method: Prompt Offset Tuning for Graph Neural Networks</h2>
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <!-- <h2 class="title is-3">Proposed Method</h2> -->
                    <div class="content has-text-justified abstract-text">
                    <div class="hero-body">
                        <video id="teaser" autoplay muted loop playsinline height="100%">
                            <source src="./static/videos/method_animation.mp4" type="video/mp4">
                        </video>
                        <!-- <h2 class="subtitle has-text-centered">
                            <span class="dnerf">POET enables users to personalize their experience by adding new action classes
                                efficiently and continually whenever they want.</span>
                        </h2> -->
                    </div>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>



    <!-- New Section: Activity Recognition Benchmark -->
    <section class="section benchmark-section">
        <div class="container is-max-desktop">
            <!-- Image Row -->
            <h2 class="section-header">Activity Recognition Benchmark</h2>
            <div class="image-gallery">
                <img src="./static/images/ntu1_avg_performance.png" alt="Image 1">
                <img src="./static/images/ntu1_old_performance.png" alt="Image 2">
                <img src="./static/images/ntu1_new_performance.png" alt="Image 3">
                <img src="./static/images/ntu1_hm_performance.png" alt="Image 4">
            </div>
    
            <!-- Content Section -->
            
            <div class="content">
                <ul>
                    <li class="list-item">Fine-tune & LWF don’t freeze the feature extractor in t>0. Entire model overfits
                        to few-shots, leading to almost complete forgetting.</li>
                    <li class="list-item">SOTA continual prompt tuning works. L2P, CODA-P fail to learn new classes. They
                        are designed for image classification, update using full supervision, assume ViT pretrained on
                        ImageNet21k.</li>
                    <li class="list-item">POET significantly outperforms upper bounds on new & is competitive on old. Best
                        stability-plasticity trade-off (HM graph).</li>
                    <li class="list-item">We show comparison with replay methods that violate privacy. FE+Replay is freezing
                        backbone + rehearsal of stored training data. POET learns New better. (Right) Removing prompts from
                        POET gives Feature Extraction (FE) – this is simply frozen feature extractor with expanding linear
                        classifier. Prompt offsets retain intermediate class performance.</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- New Section: Activity Recognition Benchmark -->
    <section class="section benchmark-section">
        <div class="container is-max-desktop">
            <!-- Image Row -->
            <h2 class="section-header">Gesture Recognition Benchmark</h2>
            <div class="image-gallery">
                <img src="./static/images/shrec_avg_performance.png" alt="Image 1">
                <img src="./static/images/shrec_old_performance.png" alt="Image 2">
                <img src="./static/images/shrec_new_performance.png" alt="Image 3">
                <img src="./static/images/shrec_hm_performance.png" alt="Image 4">
            </div>
    
            <!-- Content Section -->
            
            <div class="content">
                <ul>
                    <li class="list-item">ALICE [3] is a few-shot class-incremental baseline adapted from image classification works. It uses a non-parametric
                    classifier. It retains old classes well since it doesn’t have an expanding classifier. But it fails to learn new classes
                    well.</li>
                    <li class="list-item">POET sets SOTA on the stability-plasticity trade-off metric, Harmonic Mean.</li>
                    <li class="list-item">This dataset and model is more lightweight than the activity benchmark. Hence, its more plastic to new knowledge and
                    less stable.</li>
                    <li class="list-item">In addition to prompts, we mitigate forgetting from the classifier by freezing weights corresponding to previous classes
                    in the classifier.</li>
                </ul>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>@inproceedings{garg2024poet,
        author    = {Garg, Prachi and Joseph, K J and Balasubramanian, Vineeth N and Camgoz, Necati Cihan and Wan, Chengde and Kin, Kenrick and Si, Weiguang and Ma, Shugao and De La Torre, Fernando},
        title     = {POET: Prompt Offset Tuning for Continual Human Action Adaptation},
        booktitle   = {European Conference on Computer Vision},
        year      = {2024},
      }</code></pre>
        </div>
    </section>

</body>

</html>
